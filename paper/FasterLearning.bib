@inproceedings{VNHKH,
  author    = {Abhinav Vishnu and
               Jeyanthi Narasimhan and
               Lawrence Holder and
               Darren J. Kerbyson and
               Adolfy Hoisie},
  title     = {Fast and Accurate Support Vector Machines on Large Scale Systems},
  booktitle = {2015 {IEEE} International Conference on Cluster Computing, {CLUSTER}
               2015, Chicago, IL, USA, September 8-11, 2015},
  pages     = {110--119},
  year      = {2015},
  IGNOREcrossref  = {DBLP:conf/cluster/2015},
  url       = {http://dx.doi.org/10.1109/CLUSTER.2015.26},
  doi       = {10.1109/CLUSTER.2015.26},
  timestamp = {Thu, 05 Nov 2015 13:26:07 +0100},
  biburl    = {http://dblp.uni-trier.de/rec/bib/conf/cluster/VishnuNHKH15},
  bibsource = {dblp computer science bibliography, http://dblp.org}
}

@article{Duchi,
 author = {Duchi, John and Hazan, Elad and Singer, Yoram},
 title = {Adaptive Subgradient Methods for Online Learning and Stochastic Optimization},
 journal = {J. Mach. Learn. Res.},
 issue_date = {2/1/2011},
 volume = {12},
 month = jul,
 year = {2011},
 issn = {1532-4435},
 pages = {2121--2159},
 numpages = {39},
 url = {http://dl.acm.org/citation.cfm?id=1953048.2021068},
 acmid = {2021068},
 publisher = {JMLR.org},
} 

@MISC{Joachims98makinglarge-scale,
    author = {Thorsten Joachims},
    title = {Making Large-Scale Support Vector Machine Learning Practical},
    year = {1998}
}

@misc{tensorflow2015-whitepaper,
title={{TensorFlow}: Large-Scale Machine Learning on Heterogeneous Systems},
url={http://tensorflow.org/},
note={Software available from tensorflow.org},
author={
    Mart\'{\i}n~Abadi and
    Ashish~Agarwal and
    Paul~Barham and
    Eugene~Brevdo and
    Zhifeng~Chen and
    Craig~Citro and
    Greg~S.~Corrado and
    Andy~Davis and
    Jeffrey~Dean and
    Matthieu~Devin and
    Sanjay~Ghemawat and
    Ian~Goodfellow and
    Andrew~Harp and
    Geoffrey~Irving and
    Michael~Isard and
    Yangqing Jia and
    Rafal~Jozefowicz and
    Lukasz~Kaiser and
    Manjunath~Kudlur and
    Josh~Levenberg and
    Dan~Man\'{e} and
    Rajat~Monga and
    Sherry~Moore and
    Derek~Murray and
    Chris~Olah and
    Mike~Schuster and
    Jonathon~Shlens and
    Benoit~Steiner and
    Ilya~Sutskever and
    Kunal~Talwar and
    Paul~Tucker and
    Vincent~Vanhoucke and
    Vijay~Vasudevan and
    Fernanda~Vi\'{e}gas and
    Oriol~Vinyals and
    Pete~Warden and
    Martin~Wattenberg and
    Martin~Wicke and
    Yuan~Yu and
    Xiaoqiang~Zheng},
  year={2015},
}

@TECHREPORT{Krizhevsky09learningmultiple,
    author = {Alex Krizhevsky},
    title = {Learning multiple layers of features from tiny images},
	institution = {University of Toronto},
    year = {2009}
}

@article{ILSVRC15,
Author = {Olga Russakovsky and Jia Deng and Hao Su and Jonathan Krause and Sanjeev Satheesh and Sean Ma and Zhiheng Huang and Andrej Karpathy and Aditya Khosla and Michael Bernstein and Alexander C. Berg and Li Fei-Fei},
Title = {{ImageNet Large Scale Visual Recognition Challenge}},
Year = {2015},
journal   = {International Journal of Computer Vision (IJCV)},
doi = {10.1007/s11263-015-0816-y},
volume={115},
number={3},
pages={211-252}
}

@inproceedings{43022,
title = {Going Deeper with Convolutions},
author  = {Christian Szegedy and Wei Liu and Yangqing Jia and Pierre Sermanet and Scott Reed and Dragomir Anguelov and Dumitru Erhan and Vincent Vanhoucke and Andrew Rabinovich},
year  = 2015,
URL = {http://arxiv.org/abs/1409.4842},
booktitle = {CVPR 2015}
}

@article{adultref,
 author = {Deepajothi, S and Selvarajan, S},
 title = {A Comparative Study of Classification Techniques on Adult Data Set},
 journal = {International Journal of Engineering Research and Yechnology},
 issue_date = {10/2012},
 volume = {1},
 month = oct,
 year = {2012},
 url = {http://www.ijert.org/view-pdf/1422/a-comparative-study-of-classification-techniques-on-adult-data-set},
 issn = {2278-0181},
} 

@incollection{NIPS2014_5351,
 title = {Searching for Higgs Boson Decay Modes with Deep Learning},
 author = {Sadowski, Peter J and Whiteson, Daniel and Baldi, Pierre},
 booktitle = {Advances in Neural Information Processing Systems 27},
 editor = {Z. Ghahramani and M. Welling and C. Cortes and N.D. Lawrence and K.Q. Weinberger},
 pages = {2393--2401},
 year = {2014},
 publisher = {Curran Associates, Inc.},
 url = {http://papers.nips.cc/paper/5351-searching-for-higgs-boson-decay-modes-with-deep-learning.pdf}
}

@incollection{NIPS2012_4824,
 title = {ImageNet Classification with Deep Convolutional Neural Networks},
 author = {Alex Krizhevsky and Sutskever, Ilya and Geoffrey E. Hinton},
 booktitle = {Advances in Neural Information Processing Systems 25},
 editor = {F. Pereira and C.J.C. Burges and L. Bottou and K.Q. Weinberger},
 pages = {1097--1105},
 year = {2012},
 publisher = {Curran Associates, Inc.},
 url = {http://papers.nips.cc/paper/4824-imagenet-classification-with-deep-convolutional-neural-networks.pdf}
}

@Article{Simonyan14c,
    author       = "Simonyan, K. and Zisserman, A.",
    title        = "Very Deep Convolutional Networks for Large-Scale Image Recognition",
    journal      = "CoRR",
    volume       = "abs/1409.1556",
    year         = "2014"
}

@article{Bengio:2009:LDA:1658423.1658424,
 author = {Bengio, Yoshua},
 title = {Learning Deep Architectures for AI},
 journal = {Found. Trends Mach. Learn.},
 issue_date = {January 2009},
 volume = {2},
 number = {1},
 month = jan,
 year = {2009},
 issn = {1935-8237},
 pages = {1--127},
 numpages = {127},
 url = {http://dx.doi.org/10.1561/2200000006},
 doi = {10.1561/2200000006},
 acmid = {1658424},
 publisher = {Now Publishers Inc.},
 address = {Hanover, MA, USA},
} 

@article{Hinton:2002:TPE:639729.639730,
 author = {Hinton, Geoffrey E.},
 title = {Training Products of Experts by Minimizing Contrastive Divergence},
 journal = {Neural Comput.},
 issue_date = {August 2002},
 volume = {14},
 number = {8},
 month = aug,
 year = {2002},
 issn = {0899-7667},
 pages = {1771--1800},
 numpages = {30},
 url = {http://dx.doi.org/10.1162/089976602760128018},
 doi = {10.1162/089976602760128018},
 acmid = {639730},
 publisher = {MIT Press},
 address = {Cambridge, MA, USA},
} 

@incollection{LeCun:1998:CNI:303568.303704,
 author = {LeCun, Yann and Bengio, Yoshua},
 chapter = {Convolutional Networks for Images, Speech, and Time Series},
 title = {The Handbook of Brain Theory and Neural Networks},
 editor = {Arbib, Michael A.},
 year = {1998},
 isbn = {0-262-51102-9},
 pages = {255--258},
 numpages = {4},
 url = {http://dl.acm.org/citation.cfm?id=303568.303704},
 acmid = {303704},
 publisher = {MIT Press},
 address = {Cambridge, MA, USA},
} 

@INPROCEEDINGS{Lecun98gradient-basedlearning,
    author = {Yann Lecun and Léon Bottou and Yoshua Bengio and Patrick Haffner},
    title = {Gradient-based learning applied to document recognition},
    booktitle = {Proceedings of the IEEE},
    year = {1998},
    pages = {2278--2324}
}

@article{Suykens:1999:LSS:326394.326408,
 author = {Suykens, J. A. K. and Vandewalle, J.},
 title = {Least Squares Support Vector Machine Classifiers},
 journal = {Neural Process. Lett.},
 issue_date = {June 1999},
 volume = {9},
 number = {3},
 month = jun,
 year = {1999},
 issn = {1370-4621},
 pages = {293--300},
 numpages = {8},
 url = {http://dx.doi.org/10.1023/A:1018628609742},
 doi = {10.1023/A:1018628609742},
 acmid = {326408},
 publisher = {Kluwer Academic Publishers},
 address = {Hingham, MA, USA},
 keywords = {classification, linear least squares, radial basis function kernel, support vector machines},
} 

@ARTICLE{Tibshirani94regressionshrinkage,
    author = {Robert Tibshirani},
    title = {Regression Shrinkage and Selection Via the Lasso},
    journal = {Journal of the Royal Statistical Society, Series B},
    year = {1994},
    volume = {58},
    pages = {267--288}
}

@inproceedings{Vedaldi:2015:MCN:2733373.2807412,
 author = {Vedaldi, Andrea and Lenc, Karel},
 title = {MatConvNet: Convolutional Neural Networks for MATLAB},
 booktitle = {Proceedings of the 23rd ACM International Conference on Multimedia},
 series = {MM '15},
 year = {2015},
 isbn = {978-1-4503-3459-4},
 location = {Brisbane, Australia},
 pages = {689--692},
 numpages = {4},
 url = {http://doi.acm.org/10.1145/2733373.2807412},
 doi = {10.1145/2733373.2807412},
 acmid = {2807412},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {computer vision, convolutional neural networks, deep learning, image understanding, machine learning},
} 

@article{Vincent:2010:SDA:1756006.1953039,
 author = {Vincent, Pascal and Larochelle, Hugo and Lajoie, Isabelle and Bengio, Yoshua and Manzagol, Pierre-Antoine},
 title = {Stacked Denoising Autoencoders: Learning Useful Representations in a Deep Network with a Local Denoising Criterion},
 journal = {J. Mach. Learn. Res.},
 issue_date = {3/1/2010},
 volume = {11},
 month = dec,
 year = {2010},
 issn = {1532-4435},
 pages = {3371--3408},
 numpages = {38},
 url = {http://dl.acm.org/citation.cfm?id=1756006.1953039},
 acmid = {1953039},
 publisher = {JMLR.org},
} 

@incollection{NIPS2013_4892,
    Title = {Lasso Screening Rules via Dual Polytope Projection},
    Url = {http://media.nips.cc/nipsbooks/nipspapers/paper_files/nips26/569.pdf},
    Booktitle = {Advances in Neural Information Processing Systems 26},
    Author = {Jie Wang and Jiayu Zhou and Peter Wonka and Jieping Ye},
    Editor = {C.j.c. Burges and L. Bottou and M. Welling and Z. Ghahramani and K.q. Weinberger},
    Year = {2013},
    Pages = {1070--1078},
	  Publisher = {Neural Information Processing Systems ( NIPS )}
} 
   
@InProceedings{SalHinton07,
  author=         "Ruslan Salakhutdinov and Geoffrey Hinton",
  title=          "Deep {B}oltzmann Machines",
  booktitle=      "Proceedings of the International Conference on Artificial Intelligence and Statistics",
  volume=         "5", 
  pages=		"448-455",
  year=           "2009"
}

@MASTERSTHESIS\{IMM2012-06284,
    author       = "R. B. Palm",
    title        = "Prediction as a candidate for learning deep hierarchical models of data",
    year         = "2012",
    school       = "Technical University of Denmark, {DTU} Informatics, {E-}mail: reception@imm.dtu.dk",
    address      = "Asmussens Alle, Building 305, {DK-}2800 Kgs. Lyngby, Denmark",
    type         = "",
    note         = "Supervised by Associate Professor Ole Winther, owi@imm.dtu.dk, {DTU} Informatics, and Morten M{\o}rup, mm@imm.dtu.dk, {DTU} Informatics",
    url          = "http://www.imm.dtu.dk/English.aspx",
    abstract     = "Recent findings [HOT06] have made possible the learning of deep layered hierarchical representations of data mimicking the brains working. It is hoped that this paradigm will unlock some of the power of the brain and lead to advances towards true {AI}.
In this thesis I implement and evaluate state-of-the-art deep learning models and using these as building blocks I investigate the hypothesis that predicting the time-to-time sensory input is a good learning objective. I introduce the Predictive Encoder (PE) and show that a simple non-regularized learning rule, minimizing prediction error on natural video patches leads to receptive fields similar to those found in Macaque monkey visual area V1. I scale this model to video of natural scenes by introducing the Convolutional Predictive Encoder (CPE) and show similar results. Both models can be used in deep architectures as a deep learning module."
}
 
@techreport{manyika11frontier,
  added-at = {2013-03-15T09:48:25.000+0100},
  author = {Manyika, James and Chui, Michael and Brown, Brad and Bughin, Jacques and Dobbs, Richard and Roxburgh, Charles and Byers, Angela Hung},
  biburl = {http://www.bibsonomy.org/bibtex/2cb919811c0adab2e1d359ce1bdbac9d4/sb3000},
  institution = {McKinsey Global Institute},
  interhash = {117de3444c856242c38ab48d24604b52},
  intrahash = {cb919811c0adab2e1d359ce1bdbac9d4},
  keywords = {analytics bigdata mckinsey},
  month = {June},
  timestamp = {2013-07-01T22:02:25.000+0200},
  title = {Big Data: The Next Frontier for Innovation, Competition, and Productivity},
  year = 2011
}

@article{HintonSalakhutdinov2006b,
  abstract = {High-dimensional data can be converted to low-dimensional codes by training a multilayer neural network with a small central layer to reconstruct high-dimensional input vectors. Gradient descent can be used for fine-tuning the weights in such "autoencoder" networks, but this works well only if the initial weights are close to a good solution. We describe an effective way of initializing the weights that allows deep autoencoder networks to learn low-dimensional codes that work much better than principal components analysis as a tool to reduce the dimensionality of data.},
  added-at = {2008-07-15T10:05:18.000+0200},
  author = {Hinton, G E and Salakhutdinov, R R},
  biburl = {http://www.bibsonomy.org/bibtex/2135bbce97b449ddf5fca7be88102b53c/tmalsburg},
  description = {Reducing the dimensionality of data with neural ne...[Science. 2006] - PubMed Result},
  doi = {10.1126/science.1127647},
  interhash = {019918b82518b74f443a22dc58a0117f},
  intrahash = {135bbce97b449ddf5fca7be88102b53c},
  journal = {Science},
  keywords = {dimensionalityreduction neuralnetworks parameterestimation},
  month = Jul,
  number = 5786,
  pages = {504-507},
  pmid = {16873662},
  timestamp = {2008-07-15T10:05:18.000+0200},
  title = {Reducing the dimensionality of data with neural networks},
  url = {http://www.ncbi.nlm.nih.gov/sites/entrez?db=pubmed&uid=16873662&cmd=showdetailview&indexed=google},
  volume = 313,
  year = 2006
}

@article{gardner1985exponential,
  title={Exponential smoothing: The state of the art},
  author={Gardner, Everette S},
  journal={Journal of forecasting},
  volume={4},
  number={1},
  pages={1--28},
  year={1985},
  publisher={Wiley Online Library}
}

@article{deng2012three,
  title={Three classes of deep learning architectures and their applications: a tutorial survey},
  author={Deng, Li},
  journal={APSIPA transactions on signal and information processing},
  year={2012}
}

@article{Baldi:2014kfa,
      author         = "Baldi, Pierre and Sadowski, Peter and Whiteson, Daniel",
      title          = "{Searching for Exotic Particles in High-Energy Physics
                        with Deep Learning}",
      journal        = "Nature Commun.",
      volume         = "5",
      year           = "2014",
      pages          = "4308",
      doi            = "10.1038/ncomms5308",
      eprint         = "1402.4735",
      archivePrefix  = "arXiv",
      primaryClass   = "hep-ph",
      SLACcitation   = "%%CITATION = ARXIV:1402.4735;%%"
}

@article{Reed:1993:PAS:2325855.2328312,
 author = {Reed, R.},
 title = {Pruning Algorithms-a Survey},
 journal = {Trans. Neur. Netw.},
 issue_date = {September 1993},
 volume = {4},
 number = {5},
 month = sep,
 year = {1993},
 issn = {1045-9227},
 pages = {740--747},
 numpages = {8},
 url = {http://dx.doi.org/10.1109/72.248452},
 doi = {10.1109/72.248452},
 acmid = {2328312},
 publisher = {IEEE Press},
 address = {Piscataway, NJ, USA},
} 

@article{chambers2004simulated,
  title={Simulated apoptosis/neurogenesis regulates learning and memory capabilities of adaptive neural networks},
  author={Chambers, R Andrew and Potenza, Marc N and Hoffman, Ralph E and Miranker, Willard},
  journal={Neuropsychopharmacology},
  volume={29},
  number={4},
  pages={747--758},
  year={2004}
}

@article{ghiassi2005dynamic,
  title={A dynamic artificial neural network model for forecasting time series events},
  author={Ghiassi, M and Saidane, H and Zimbra, DK},
  journal={International Journal of Forecasting},
  volume={21},
  number={2},
  pages={341--362},
  year={2005},
  publisher={Elsevier}
}

@article{leuner2006there,
  title={Is there a link between adult neurogenesis and learning?},
  author={Leuner, Benedetta and Gould, Elizabeth and Shors, Tracey J},
  journal={Hippocampus},
  volume={16},
  number={3},
  pages={216--224},
  year={2006},
  publisher={Wiley Online Library}
}

@inproceedings{sietsma1988neural,
  title={Neural net pruning-why and how},
  author={Sietsma, Jocelyn and Dow, Robert JF},
  booktitle={Neural Networks, 1988., IEEE International Conference on},
  pages={325--333},
  year={1988},
  organization={IEEE}
}

@inproceedings{gorban1999generation,
  title={Generation of explicit knowledge from empirical data through pruning of trainable neural networks},
  author={Gorban, Alexander N and Mirkes, Eugeniy M and Tsaregorodtsev, Victor G},
  booktitle={Neural Networks, 1999. IJCNN'99. International Joint Conference on},
  volume={6},
  pages={4393--4398},
  year={1999},
  organization={IEEE}
}

@article{aimone2009computational,
  title={Computational influence of adult neurogenesis on memory encoding},
  author={Aimone, James B and Wiles, Janet and Gage, Fred H},
  journal={Neuron},
  volume={61},
  number={2},
  pages={187--202},
  year={2009},
  publisher={Elsevier}
}

@inproceedings{VNHKH,
  author    = {Abhinav Vishnu and
               Jeyanthi Narasimhan and
               Lawrence Holder and
               Darren J. Kerbyson and
               Adolfy Hoisie},
  title     = {Fast and Accurate Support Vector Machines on Large Scale Systems},
  booktitle = {2015 {IEEE} International Conference on Cluster Computing, {CLUSTER}
               2015, Chicago, IL, USA, September 8-11, 2015},
  pages     = {110--119},
  year      = {2015},
  IGNOREcrossref  = {DBLP:conf/cluster/2015},
  url       = {http://dx.doi.org/10.1109/CLUSTER.2015.26},
  doi       = {10.1109/CLUSTER.2015.26},
  timestamp = {Thu, 05 Nov 2015 13:26:07 +0100},
  biburl    = {http://dblp.uni-trier.de/rec/bib/conf/cluster/VishnuNHKH15},
  bibsource = {dblp computer science bibliography, http://dblp.org}
}

@article{Hinton:2002:TPE:639729.639730,
 author = {Hinton, Geoffrey E.},
 title = {Training Products of Experts by Minimizing Contrastive Divergence},
 journal = {Neural Comput.},
 issue_date = {August 2002},
 volume = {14},
 number = {8},
 month = aug,
 year = {2002},
 issn = {0899-7667},
 pages = {1771--1800},
 numpages = {30},
 url = {http://dx.doi.org/10.1162/089976602760128018},
 doi = {10.1162/089976602760128018},
 acmid = {639730},
 publisher = {MIT Press},
 address = {Cambridge, MA, USA},
} 

@INPROCEEDINGS{Lecun98gradient-basedlearning,
    author = {Yann Lecun and Léon Bottou and Yoshua Bengio and Patrick Haffner},
    title = {Gradient-based learning applied to document recognition},
    booktitle = {Proceedings of the IEEE},
    year = {1998},
    pages = {2278--2324}
}

@article{Suykens:1999:LSS:326394.326408,
 author = {Suykens, J. A. K. and Vandewalle, J.},
 title = {Least Squares Support Vector Machine Classifiers},
 journal = {Neural Process. Lett.},
 issue_date = {June 1999},
 volume = {9},
 number = {3},
 month = jun,
 year = {1999},
 issn = {1370-4621},
 pages = {293--300},
 numpages = {8},
 url = {http://dx.doi.org/10.1023/A:1018628609742},
 doi = {10.1023/A:1018628609742},
 acmid = {326408},
 publisher = {Kluwer Academic Publishers},
 address = {Hingham, MA, USA},
 keywords = {classification, linear least squares, radial basis function kernel, support vector machines},
} 

@inproceedings{Vedaldi:2015:MCN:2733373.2807412,
 author = {Vedaldi, Andrea and Lenc, Karel},
 title = {MatConvNet: Convolutional Neural Networks for MATLAB},
 booktitle = {Proceedings of the 23rd ACM International Conference on Multimedia},
 series = {MM '15},
 year = {2015},
 isbn = {978-1-4503-3459-4},
 location = {Brisbane, Australia},
 pages = {689--692},
 numpages = {4},
 url = {http://doi.acm.org/10.1145/2733373.2807412},
 doi = {10.1145/2733373.2807412},
 acmid = {2807412},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {computer vision, convolutional neural networks, deep learning, image understanding, machine learning},
} 

 
@MASTERSTHESIS\{IMM2012-06284,
    author       = "R. B. Palm",
    title        = "Prediction as a candidate for learning deep hierarchical models of data",
    year         = "2012",
    school       = "Technical University of Denmark, {DTU} Informatics, {E-}mail: reception@imm.dtu.dk",
    address      = "Asmussens Alle, Building 305, {DK-}2800 Kgs. Lyngby, Denmark",
    type         = "",
    note         = "Supervised by Associate Professor Ole Winther, owi@imm.dtu.dk, {DTU} Informatics, and Morten M{\o}rup, mm@imm.dtu.dk, {DTU} Informatics",
    url          = "http://www.imm.dtu.dk/English.aspx",
    abstract     = "Recent findings [HOT06] have made possible the learning of deep layered hierarchical representations of data mimicking the brains working. It is hoped that this paradigm will unlock some of the power of the brain and lead to advances towards true {AI}.
In this thesis I implement and evaluate state-of-the-art deep learning models and using these as building blocks I investigate the hypothesis that predicting the time-to-time sensory input is a good learning objective. I introduce the Predictive Encoder (PE) and show that a simple non-regularized learning rule, minimizing prediction error on natural video patches leads to receptive fields similar to those found in Macaque monkey visual area V1. I scale this model to video of natural scenes by introducing the Convolutional Predictive Encoder (CPE) and show similar results. Both models can be used in deep architectures as a deep learning module."
}
 
@techreport{manyika11frontier,
  added-at = {2013-03-15T09:48:25.000+0100},
  author = {Manyika, James and Chui, Michael and Brown, Brad and Bughin, Jacques and Dobbs, Richard and Roxburgh, Charles and Byers, Angela Hung},
  biburl = {http://www.bibsonomy.org/bibtex/2cb919811c0adab2e1d359ce1bdbac9d4/sb3000},
  institution = {McKinsey Global Institute},
  interhash = {117de3444c856242c38ab48d24604b52},
  intrahash = {cb919811c0adab2e1d359ce1bdbac9d4},
  keywords = {analytics bigdata mckinsey},
  month = {June},
  timestamp = {2013-07-01T22:02:25.000+0200},
  title = {Big Data: The Next Frontier for Innovation, Competition, and Productivity},
  year = 2011
}

@article{gardner1985exponential,
  title={Exponential smoothing: The state of the art},
  author={Gardner, Everette S},
  journal={Journal of forecasting},
  volume={4},
  number={1},
  pages={1--28},
  year={1985},
  publisher={Wiley Online Library}
}

@article{deng2012three,
  title={Three classes of deep learning architectures and their applications: a tutorial survey},
  author={Deng, Li},
  journal={APSIPA transactions on signal and information processing},
  year={2012}
}

@article{DBLP:journals/corr/abs-1009-4983,
  author    = {S. M. Kamruzzaman and
               Ahmed Ryadh Hasan},
  title     = {Pattern Classification using Simplified Neural Networks},
  journal   = {CoRR},
  volume    = {abs/1009.4983},
  year      = {2010},
  url       = {http://arxiv.org/abs/1009.4983},
  timestamp = {Mon, 05 Dec 2011 18:04:05 +0100},
  biburl    = {http://dblp.uni-trier.de/rec/bib/journals/corr/abs-1009-4983},
  bibsource = {dblp computer science bibliography, http://dblp.org}
}

@inproceedings{goodrich2014neuron,
  title={Neuron clustering for mitigating catastrophic forgetting in feedforward neural networks},
  author={Goodrich, Ben and Arel, Itamar},
  booktitle={Computational Intelligence in Dynamic and Uncertain Environments (CIDUE), 2014 IEEE Symposium on},
  pages={62--68},
  year={2014},
  organization={IEEE}
}

@ARTICLE{2016arXiv160302339V,
   author = {{Vishnu}, A. and {Siegel}, C. and {Daily}, J.},
    title = "{Distributed TensorFlow with MPI}",
  journal = {ArXiv e-prints},
archivePrefix = "arXiv",
   eprint = {1603.02339},
 primaryClass = "cs.DC",
 keywords = {Computer Science - Distributed, Parallel, and Cluster Computing},
     year = 2016,
    month = mar,
   adsurl = {http://adsabs.harvard.edu/abs/2016arXiv160302339V},
  adsnote = {Provided by the SAO/NASA Astrophysics Data System}
}

@article{Wang:1993:GCC:155870.155881,
 author = {Wang, Xingwei and Zhao, Hong and Zhu, Jiakeng},
 title = {GRPC: A Communication Cooperation Mechanism in Distributed Systems},
 journal = {SIGOPS Oper. Syst. Rev.},
 issue_date = {July 1993},
 volume = {27},
 number = {3},
 month = jul,
 year = {1993},
 issn = {0163-5980},
 pages = {75--86},
 numpages = {12},
 url = {http://doi.acm.org/10.1145/155870.155881},
 doi = {10.1145/155870.155881},
 acmid = {155881},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@TECHREPORT{citeulike:13837097,
    author = {Steve Lawrence and C. Lee Giles and Ah Chung Tsoi},
    title = {What size neural network gives optimal generalization? convergence properties of backpropagation},
    institution = {NEC Research Institute and University of Queensland},
    year = {1996}
}

@incollection{NIPS2006_3048,
title = {Greedy Layer-Wise Training of Deep Networks},
author = {Bengio, Yoshua and Pascal Lamblin and Dan Popovici and Larochelle, Hugo},
booktitle = {Advances in Neural Information Processing Systems 19},
editor = {B. Sch\"{o}lkopf and J. C. Platt and T. Hoffman},
pages = {153--160},
year = {2007},
publisher = {MIT Press},
url = {http://papers.nips.cc/paper/3048-greedy-layer-wise-training-of-deep-networks.pdf}
}

@ARTICLE{Hinton06afast,
    author = {Geoffrey E. Hinton and Simon Osindero},
    title = {A fast learning algorithm for deep belief nets},
    journal = {Neural Computation},
    year = {2006},
    volume = {18},
    pages = {2006}
}

@article{Bianchini2014,
	abstract = {Recently, researchers in the artificial neural network field have focused their attention on connectionist models composed by several hidden layers. In fact, experimental results and heuristic considerations suggest that deep architectures are more suitable than shallow ones for modern applications, facing very complex problems, e.g., vision and human language understanding. However, the actual theoretical results supporting such a claim are still few and incomplete. In this paper, we propose a new approach to study how the depth of feedforward neural networks impacts on their ability in implementing high complexity functions. First, a new measure based on topological concepts is introduced, aimed at evaluating the complexity of the function implemented by a neural network, used for classification purposes. Then, deep and shallow neural architectures with common sigmoidal activation functions are compared, by deriving upper and lower bounds on their complexity, and studying how the complexity depends on the number of hidden units and the used activation function. The obtained results seem to support the idea that deep networks actually implements functions of higher complexity, so that they are able, with the same number of resources, to address more difficult problems.},
	affiliation = {, University of Siena, Siena, Italy},
	author = {Bianchini, Monica and Scarselli, Franco},
	doi = {10.1109/TNNLS.2013.2293637},
	journal = {IEEE Transactions on Neural Networks and Learning Systems},
	keywords = {Complexity theory; Neurons; Biological neural networks; Polynomials; Upper bound; Computer architecture; Vapnik--Chervonenkis dimension (VC-dim).; Betti numbers; deep neural networks; function approximation; topological complexity},
	language = {Undetermined},
	number = {8},
	pages = {1553 - 1565},
	title = {On the Complexity of Neural Network Classifiers: A Comparison Between Shallow and Deep Architectures},
	volume = {25},
	year = {2014},
}

@Article{ref1,
author="Cybenko, G.",
title="Approximation by superpositions of a sigmoidal function",
journal="Mathematics of Control, Signals and Systems",
volume="2",
number="4",
pages="303--314",
abstract="In this paper we demonstrate that finite linear combinations of compositions of a fixed, univariate function and a set of affine functionals can uniformly approximate any continuous function ofn real variables with support in the unit hypercube; only mild conditions are imposed on the univariate function. Our results settle an open question about representability in the class of single hidden layer neural networks. In particular, we show that arbitrary decision regions can be arbitrarily well approximated by continuous feedforward neural networks with only a single internal, hidden layer and any continuous sigmoidal nonlinearity. The paper discusses approximation properties of other possible types of nonlinearities that might be implemented by artificial neural networks.",
issn="1435-568X",
doi="10.1007/BF02551274",
url="http://dx.doi.org/10.1007/BF02551274",
year="1989"
}

@article{hubel1968receptive,
  title={Receptive fields and functional architecture of monkey striate cortex},
  author={Hubel, David H and Wiesel, Torsten N},
  journal={The Journal of physiology},
  volume={195},
  number={1},
  pages={215--243},
  year={1968},
  publisher={Wiley Online Library}
}

@article{hinton2012improving,
  title={Improving neural networks by preventing co-adaptation of feature detectors},
  author={Hinton, Geoffrey E and Srivastava, Nitish and Krizhevsky, Alex and Sutskever, Ilya and Salakhutdinov, Ruslan R},
  journal={arXiv preprint arXiv:1207.0580},
  year={2012}
}

@inproceedings{lee2009convolutional,
  title={Convolutional deep belief networks for scalable unsupervised learning of hierarchical representations},
  author={Lee, Honglak and Grosse, Roger and Ranganath, Rajesh and Ng, Andrew Y},
  booktitle={Proceedings of the 26th Annual International Conference on Machine Learning},
  pages={609--616},
  year={2009},
  organization={ACM}
}

@article{lecun1998gradient,
  title={Gradient-based learning applied to document recognition},
  author={LeCun, Yann and Bottou, L{\'e}on and Bengio, Yoshua and Haffner, Patrick},
  journal={Proceedings of the IEEE},
  volume={86},
  number={11},
  pages={2278--2324},
  year={1998},
  publisher={IEEE}
}

@article{jia2014caffe,
  Author = {Jia, Yangqing and Shelhamer, Evan and Donahue, Jeff and Karayev, Sergey and Long, Jonathan and Girshick, Ross and Guadarrama, Sergio and Darrell, Trevor},
  Journal = {arXiv preprint arXiv:1408.5093},
  Title = {Caffe: Convolutional Architecture for Fast Feature Embedding},
  Year = {2014}
}

@MISC{Bastien-Theano-2012,
        author = {Bastien, Fr{\'{e}}d{\'{e}}ric and Lamblin, Pascal and Pascanu, Razvan and Bergstra, James and Goodfellow, Ian J. and Bergeron, Arnaud and Bouchard, Nicolas and Bengio, Yoshua},
         title = {Theano: new features and speed improvements},
          year = {2012},
  howpublished = {Deep Learning and Unsupervised Feature Learning NIPS 2012 Workshop},
      abstract = {Theano is a linear algebra compiler that optimizes a user’s symbolically-speciﬁed
mathematical computations to produce efﬁcient low-level implementations. In
this paper, we present new features and efﬁciency improvements to Theano, and
benchmarks demonstrating Theano’s performance relative to Torch7, a recently
introduced machine learning library, and to RNNLM, a C++ library targeted at
recurrent neural networks.}
}

@INPROCEEDINGS{bergstra+al:2010-scipy,
     author = {Bergstra, James and Breuleux, Olivier and Bastien, Fr{\'{e}}d{\'{e}}ric and Lamblin, Pascal and Pascanu, Razvan and Desjardins, Guillaume and Turian, Joseph and Warde-Farley, David and Bengio, Yoshua},
      month = jun,
      title = {Theano: a {CPU} and {GPU} Math Expression Compiler},
  booktitle = {Proceedings of the Python for Scientific Computing Conference ({SciPy})},
       year = {2010},
   location = {Austin, TX},
       note = {Oral Presentation},
   abstract = {Theano is a compiler for mathematical expressions in Python that combines the convenience of NumPy’s syntax with the speed of optimized native machine language. The user composes mathematical expressions in a high-level description that mimics NumPy’s syntax and semantics, while being statically typed and
functional (as opposed to imperative). These expressions allow Theano to provide symbolic differentiation. Before performing computation, Theano optimizes the choice of expressions, translates
them into C++ (or CUDA for GPU), compiles them into dynamically loaded Python modules, all automatically. Common machine learning algorithms implemented with Theano are from 1.6× to 7.5× faster than competitive alternatives (including those implemented with C/C++, NumPy/SciPy and MATLAB) when compiled for the
CPU and between 6.5× and 44× faster when compiled for the GPU. This paper illustrates how to use Theano, outlines the scope of the compiler, provides benchmarks on both CPU and GPU processors, and explains its overall design.}
}

@TechReport {export:226641,
abstract     = {<p>We introduce computational network (CN), a unified framework for describing
                arbitrary learning machines, such as deep neural networks (DNNs), convolutional
                neural networks (CNNs), recurrent neural networks (RNNs), long short term memory
                (LSTM), logistic regression, and maximum entropy model, that can be illustrated
                as a series of computational steps. A CN is a directed graph in which each leaf
                node represents an input value or a parameter and each non-leaf node represents a
                matrix operation upon its children. We describe algorithms to carry out forward
                computation and gradient calculation in CN and introduce most popular computation
                node types used in a typical CN.
We further introduce the computational network
                toolkit (CNTK), an implementation of CN that supports both GPU and CPU. We
                describe the architecture and the key components of the CNTK, the command line
                options to use CNTK, and the network definition and model editing language, and
                provide sample setups for acoustic model, language model, and spoken language
                understanding. We also describe the Argon speech recognition decoder as an
                example to integrate with CNTK.</p>},
author       = {Amit Agarwal and Eldar Akchurin and Chris Basoglu and Guoguo Chen and Scott
                Cyphers and Jasha Droppo and Adam Eversole and Brian Guenter and Mark Hillebrand
                and Ryan Hoens and Xuedong Huang and Zhiheng Huang and Vladimir Ivanov and Alexey
                Kamenev and Philipp Kranen and Oleksii Kuchaiev and Wolfgang Manousek and Avner
                May and Bhaskar Mitra and Olivier Nano and Gaizka Navarro and Alexey Orlov and
                Marko Padmilac and Hari Parthasarathi and Baolin Peng and Alexey Reznichenko and
                Frank Seide and Michael L. Seltzer and Malcolm Slaney and Andreas Stolcke and
                Yongqiang Wang and Huaming Wang and Kaisheng Yao and Dong Yu and Yu Zhang and
                Geoffrey Zweig},
month        = {August},
number       = {MSR-TR-2014-112},
publisher    = {Microsoft Research},
title        = {An Introduction to Computational Networks and the Computational Network Toolkit},
url          = {http://research.microsoft.com/apps/pubs/default.aspx?id=226641},
year         = {2014},
}

@MISC{Collobert02torch:a,
    author = {Ronan Collobert and Samy Bengio and Johnny Marithoz},
    title = {Torch: A Modular Machine Learning Software Library},
    year = {2002}
}

@inproceedings{freund1995desicion,
  title={A desicion-theoretic generalization of on-line learning and an application to boosting},
  author={Freund, Yoav and Schapire, Robert E},
  booktitle={Computational learning theory},
  pages={23--37},
  year={1995},
  organization={Springer}
}

@article{Freund1995256,
title = "Boosting a Weak Learning Algorithm by Majority ",
journal = "Information and Computation ",
volume = "121",
number = "2",
pages = "256 - 285",
year = "1995",
note = "",
issn = "0890-5401",
doi = "http://dx.doi.org/10.1006/inco.1995.1136",
url = "http://www.sciencedirect.com/science/article/pii/S0890540185711364",
author = "Y. Freund"
}

@article{agarwal2005geometric,
  title={Geometric approximation via coresets},
  author={Agarwal, Pankaj K and Har-Peled, Sariel and Varadarajan, Kasturi R},
  journal={Combinatorial and computational geometry},
  volume={52},
  pages={1--30},
  year={2005},
  publisher={Cambridge University Press, New York}
}

@techreport{settles.tr09,
  Author = {Burr Settles},
  Institution = {University of Wisconsin--Madison},
  Number = {1648},
  Title = {Active Learning Literature Survey},
  Type = {Computer Sciences Technical Report},
  Year = {2009},
}

@CONFERENCE{Lowe19991150,
author={Lowe, David G.},
title={Object recognition from local scale-invariant features},
journal={Proceedings of the IEEE International Conference on Computer Vision},
year={1999},
volume={2},
pages={1150-1157},
note={cited By 4840},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-0033284915&partnerID=40&md5=38d9eab59c3f59561877a883fd6148d0},
document_type={Conference Paper},
source={Scopus},
}

@article{Stallkamp2012323,
title = "Man vs. computer: Benchmarking machine learning algorithms for traffic sign recognition ",
journal = "Neural Networks ",
volume = "32",
number = "",
pages = "323 - 332",
year = "2012",
note = "Selected Papers from \{IJCNN\} 2011 ",
issn = "0893-6080",
doi = "http://dx.doi.org/10.1016/j.neunet.2012.02.016",
url = "http://www.sciencedirect.com/science/article/pii/S0893608012000457",
author = "J. Stallkamp and M. Schlipsing and J. Salmen and C. Igel",
keywords = "Traffic sign recognition",
keywords = "Machine learning",
keywords = "Convolutional neural networks",
keywords = "Benchmarking "
}

@article{Baldi198953,
title = "Neural networks and principal component analysis: Learning from examples without local minima ",
journal = "Neural Networks ",
volume = "2",
number = "1",
pages = "53 - 58",
year = "1989",
note = "",
issn = "0893-6080",
doi = "http://dx.doi.org/10.1016/0893-6080(89)90014-2",
url = "http://www.sciencedirect.com/science/article/pii/0893608089900142",
author = "Pierre Baldi and Kurt Hornik",
keywords = "Neural networks",
keywords = "Principal component analysis",
keywords = "Learning",
keywords = "Back propagation "
}

@inproceedings{tsang2005core,
  title={Core vector machines: Fast SVM training on very large data sets},
  author={Tsang, Ivor W and Kwok, James T and Cheung, Pak-Ming},
  booktitle={Journal of Machine Learning Research},
  pages={363--392},
  year={2005}
}

@inproceedings{Feldman:2013:TBD:2627817.2627920,
 author = {Feldman, Dan and Schmidt, Melanie and Sohler, Christian},
 title = {Turning Big Data into Tiny Data: Constant-size Coresets for K-means, PCA and Projective Clustering},
 booktitle = {Proceedings of the Twenty-Fourth Annual ACM-SIAM Symposium on Discrete Algorithms},
 series = {SODA '13},
 year = {2013},
 isbn = {978-1-611972-51-1},
 location = {New Orleans, Louisiana},
 pages = {1434--1453},
 numpages = {20},
 url = {http://dl.acm.org/citation.cfm?id=2627817.2627920},
 acmid = {2627920},
 publisher = {SIAM},
} 

@inproceedings{feldman2012effective,
  title={An effective coreset compression algorithm for large scale sensor networks},
  author={Feldman, Dan and Sugaya, Andrew and Rus, Daniela},
  booktitle={Information Processing in Sensor Networks (IPSN), 2012 ACM/IEEE 11th International Conference on},
  pages={257--268},
  year={2012},
  organization={IEEE}
}

@inproceedings{mason1999boosting,
  title={Boosting algorithms as gradient descent in function space},
  author={Mason, Llew and Baxter, Jonathan and Bartlett, Peter and Frean, Marcus},
  year={1999},
  organization={NIPS}
}

@article{mnistlecun,
  title={The MNIST database of handwritten digits, 1998},
  author={LeCun, Yann and Cortes, Corinna and Burges, Christopher JC},
  journal={Available electronically at http://yann.lecun.com/exdb/mnist},
  year={2012}
}

@inproceedings{ding2015yinyang,
  title={Yinyang k-means: A drop-in replacement of the classic k-means with consistent speedup},
  author={Ding, Yufei and Zhao, Yue and Shen, Xipeng and Musuvathi, Madanlal and Mytkowicz, Todd},
  booktitle={Proceedings of the 32nd International Conference on Machine Learning (ICML-15)},
  pages={579--587},
  year={2015}
}

@article{lapedriza2013all,
  title={Are all training examples equally valuable?},
  author={Lapedriza, Agata and Pirsiavash, Hamed and Bylinskii, Zoya and Torralba, Antonio},
  journal={arXiv preprint arXiv:1311.6510},
  year={2013}
}

@article{cheng2015semantically,
  title={Semantically-driven automatic creation of training sets for object recognition},
  author={Cheng, Dong Seon and Setti, Francesco and Zeni, Nicola and Ferrario, Roberta and Cristani, Marco},
  journal={Computer Vision and Image Understanding},
  volume={131},
  pages={56--71},
  year={2015},
  publisher={Elsevier}
}

@inproceedings{angelova2005pruning,
  title={Pruning training sets for learning of object categories},
  author={Angelova, Anelia and Abu-Mostafam, Yaser and Perona, Pietro},
  booktitle={2005 IEEE Computer Society Conference on Computer Vision and Pattern Recognition (CVPR'05)},
  volume={1},
  pages={494--501},
  year={2005},
  organization={IEEE}
}

@inproceedings{elhamifar2012see,
  title={See all by looking at a few: Sparse modeling for finding representative objects},
  author={Elhamifar, Ehsan and Sapiro, Guillermo and Vidal, Rene},
  booktitle={Computer Vision and Pattern Recognition (CVPR), 2012 IEEE Conference on},
  pages={1600--1607},
  year={2012},
  organization={IEEE}
}

@article{blum1997selection,
  title={Selection of relevant features and examples in machine learning},
  author={Blum, Avrim L and Langley, Pat},
  journal={Artificial intelligence},
  volume={97},
  number={1},
  pages={245--271},
  year={1997},
  publisher={Elsevier}
}

@article{sangineto2016self,
  title={Self Paced Deep Learning for Weakly Supervised Object Detection},
  author={Sangineto, Enver and Nabi, Moin and Culibrk, Dubravko and Sebe, Nicu},
  journal={arXiv preprint arXiv:1605.07651},
  year={2016}
}

@inproceedings{bengio2009curriculum,
  title={Curriculum learning},
  author={Bengio, Yoshua and Louradour, J{\'e}r{\^o}me and Collobert, Ronan and Weston, Jason},
  booktitle={Proceedings of the 26th annual international conference on machine learning},
  pages={41--48},
  year={2009},
  organization={ACM}
}

@article{ehinger2011estimating,
  title={Estimating scene typicality from human ratings and image features},
  author={Ehinger, Krista A and Xiao, Jianxiong and Torralba, Antonio and Oliva, Aude},
  year={2011},
  publisher={Cognitive Science Society, Inc.},
  journal={Proceedings of the 33rd Annual Cognitive Science Conference, COGSCI 2011, Boston, Massachusetts, Wednesday, July 20 - Saturday July 23, 2011}
}

@inproceedings{malisiewicz2011ensemble,
  title={Ensemble of exemplar-svms for object detection and beyond},
  author={Malisiewicz, Tomasz and Gupta, Abhinav and Efros, Alexei A},
  booktitle={2011 International Conference on Computer Vision},
  pages={89--96},
  year={2011},
  organization={IEEE}
}

@book{marsland2015machine,
  title={Machine learning: an algorithmic perspective},
  author={Marsland, Stephen},
  year={2015},
  publisher={CRC press}
}

@inproceedings{siegel2016adaptive,
  title={Adaptive neuron apoptosis for accelerating deep learning on large scale systems},
  author={Siegel, Charles and Daily, Jeff and Vishnu, Abhinav},
  booktitle={Big Data (Big Data), 2016 IEEE International Conference on},
  pages={753--762},
  year={2016},
  organization={IEEE}
}

@article{liu2016application,
  title={Application of Deep Convolutional Neural Networks for Detecting Extreme Weather in Climate Datasets},
  author={Liu, Yunjie and Racah, Evan and Prabhat and Correa, Joaquin and Khosrowshahi, Amir and Lavers, David and Kunkel, Kenneth and Wehner, Michael and Collins, William},
  journal={arXiv preprint arXiv:1605.01156},
  year={2016}
}

@article{frenay2014classification,
  title={Classification in the presence of label noise: a survey},
  author={Fr{\'e}nay, Beno{\^\i}t and Verleysen, Michel},
  journal={IEEE transactions on neural networks and learning systems},
  volume={25},
  number={5},
  pages={845--869},
  year={2014},
  publisher={IEEE}
}

@misc{hochreiter2001gradient,
  title={Gradient flow in recurrent nets: the difficulty of learning long-term dependencies},
  author={Hochreiter, Sepp and Bengio, Yoshua and Frasconi, Paolo and Schmidhuber, J{\"u}rgen},
  year={2001},
  publisher={A field guide to dynamical recurrent neural networks. IEEE Press}
}



@article{inception,
	  author    = {Christian Szegedy and
		                 Vincent Vanhoucke and
					                Sergey Ioffe and
							               Jonathon Shlens and
								                      Zbigniew Wojna},
	    title     = {Rethinking the Inception Architecture for Computer Vision},
	      journal   = {CoRR},
	        volume    = {abs/1512.00567},
		  year      = {2015},
		    url       = {http://arxiv.org/abs/1512.00567},
		      timestamp = {Sat, 02 Jan 2016 11:38:49 +0100},
		        biburl    = {http://dblp.uni-trier.de/rec/bib/journals/corr/SzegedyVISW15},
			  bibsource = {dblp computer science bibliography, http://dblp.org}
}

@inproceedings{tan2018survey,
  title={A survey on deep transfer learning},
  author={Tan, Chuanqi and Sun, Fuchun and Kong, Tao and Zhang, Wenchang and Yang, Chao and Liu, Chunfang},
  booktitle={International conference on artificial neural networks},
  pages={270--279},
  year={2018},
  organization={Springer}
}

@article{zhuang2020comprehensive,
  title={A comprehensive survey on transfer learning},
  author={Zhuang, Fuzhen and Qi, Zhiyuan and Duan, Keyu and Xi, Dongbo and Zhu, Yongchun and Zhu, Hengshu and Xiong, Hui and He, Qing},
  journal={Proceedings of the IEEE},
  volume={109},
  number={1},
  pages={43--76},
  year={2020},
  publisher={IEEE}
}
